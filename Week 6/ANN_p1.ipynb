{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "29589f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn # ngambil neural network, ngambil loss function\n",
    "import torch.optim as optim # Ngambil optimizer\n",
    "from sklearn.datasets import load_iris \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "031e5712",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "8980ca7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.22222222, 0.625     , 0.06779661, 0.04166667],\n",
       "       [0.16666667, 0.41666667, 0.06779661, 0.04166667],\n",
       "       [0.11111111, 0.5       , 0.05084746, 0.04166667],\n",
       "       [0.08333333, 0.45833333, 0.08474576, 0.04166667],\n",
       "       [0.19444444, 0.66666667, 0.06779661, 0.04166667],\n",
       "       [0.30555556, 0.79166667, 0.11864407, 0.125     ],\n",
       "       [0.08333333, 0.58333333, 0.06779661, 0.08333333],\n",
       "       [0.19444444, 0.58333333, 0.08474576, 0.04166667],\n",
       "       [0.02777778, 0.375     , 0.06779661, 0.04166667],\n",
       "       [0.16666667, 0.45833333, 0.08474576, 0.        ],\n",
       "       [0.30555556, 0.70833333, 0.08474576, 0.04166667],\n",
       "       [0.13888889, 0.58333333, 0.10169492, 0.04166667],\n",
       "       [0.13888889, 0.41666667, 0.06779661, 0.        ],\n",
       "       [0.        , 0.41666667, 0.01694915, 0.        ],\n",
       "       [0.41666667, 0.83333333, 0.03389831, 0.04166667],\n",
       "       [0.38888889, 1.        , 0.08474576, 0.125     ],\n",
       "       [0.30555556, 0.79166667, 0.05084746, 0.125     ],\n",
       "       [0.22222222, 0.625     , 0.06779661, 0.08333333],\n",
       "       [0.38888889, 0.75      , 0.11864407, 0.08333333],\n",
       "       [0.22222222, 0.75      , 0.08474576, 0.08333333],\n",
       "       [0.30555556, 0.58333333, 0.11864407, 0.04166667],\n",
       "       [0.22222222, 0.70833333, 0.08474576, 0.125     ],\n",
       "       [0.08333333, 0.66666667, 0.        , 0.04166667],\n",
       "       [0.22222222, 0.54166667, 0.11864407, 0.16666667],\n",
       "       [0.13888889, 0.58333333, 0.15254237, 0.04166667],\n",
       "       [0.19444444, 0.41666667, 0.10169492, 0.04166667],\n",
       "       [0.19444444, 0.58333333, 0.10169492, 0.125     ],\n",
       "       [0.25      , 0.625     , 0.08474576, 0.04166667],\n",
       "       [0.25      , 0.58333333, 0.06779661, 0.04166667],\n",
       "       [0.11111111, 0.5       , 0.10169492, 0.04166667],\n",
       "       [0.13888889, 0.45833333, 0.10169492, 0.04166667],\n",
       "       [0.30555556, 0.58333333, 0.08474576, 0.125     ],\n",
       "       [0.25      , 0.875     , 0.08474576, 0.        ],\n",
       "       [0.33333333, 0.91666667, 0.06779661, 0.04166667],\n",
       "       [0.16666667, 0.45833333, 0.08474576, 0.04166667],\n",
       "       [0.19444444, 0.5       , 0.03389831, 0.04166667],\n",
       "       [0.33333333, 0.625     , 0.05084746, 0.04166667],\n",
       "       [0.16666667, 0.66666667, 0.06779661, 0.        ],\n",
       "       [0.02777778, 0.41666667, 0.05084746, 0.04166667],\n",
       "       [0.22222222, 0.58333333, 0.08474576, 0.04166667],\n",
       "       [0.19444444, 0.625     , 0.05084746, 0.08333333],\n",
       "       [0.05555556, 0.125     , 0.05084746, 0.08333333],\n",
       "       [0.02777778, 0.5       , 0.05084746, 0.04166667],\n",
       "       [0.19444444, 0.625     , 0.10169492, 0.20833333],\n",
       "       [0.22222222, 0.75      , 0.15254237, 0.125     ],\n",
       "       [0.13888889, 0.41666667, 0.06779661, 0.08333333],\n",
       "       [0.22222222, 0.75      , 0.10169492, 0.04166667],\n",
       "       [0.08333333, 0.5       , 0.06779661, 0.04166667],\n",
       "       [0.27777778, 0.70833333, 0.08474576, 0.04166667],\n",
       "       [0.19444444, 0.54166667, 0.06779661, 0.04166667],\n",
       "       [0.75      , 0.5       , 0.62711864, 0.54166667],\n",
       "       [0.58333333, 0.5       , 0.59322034, 0.58333333],\n",
       "       [0.72222222, 0.45833333, 0.66101695, 0.58333333],\n",
       "       [0.33333333, 0.125     , 0.50847458, 0.5       ],\n",
       "       [0.61111111, 0.33333333, 0.61016949, 0.58333333],\n",
       "       [0.38888889, 0.33333333, 0.59322034, 0.5       ],\n",
       "       [0.55555556, 0.54166667, 0.62711864, 0.625     ],\n",
       "       [0.16666667, 0.16666667, 0.38983051, 0.375     ],\n",
       "       [0.63888889, 0.375     , 0.61016949, 0.5       ],\n",
       "       [0.25      , 0.29166667, 0.49152542, 0.54166667],\n",
       "       [0.19444444, 0.        , 0.42372881, 0.375     ],\n",
       "       [0.44444444, 0.41666667, 0.54237288, 0.58333333],\n",
       "       [0.47222222, 0.08333333, 0.50847458, 0.375     ],\n",
       "       [0.5       , 0.375     , 0.62711864, 0.54166667],\n",
       "       [0.36111111, 0.375     , 0.44067797, 0.5       ],\n",
       "       [0.66666667, 0.45833333, 0.57627119, 0.54166667],\n",
       "       [0.36111111, 0.41666667, 0.59322034, 0.58333333],\n",
       "       [0.41666667, 0.29166667, 0.52542373, 0.375     ],\n",
       "       [0.52777778, 0.08333333, 0.59322034, 0.58333333],\n",
       "       [0.36111111, 0.20833333, 0.49152542, 0.41666667],\n",
       "       [0.44444444, 0.5       , 0.6440678 , 0.70833333],\n",
       "       [0.5       , 0.33333333, 0.50847458, 0.5       ],\n",
       "       [0.55555556, 0.20833333, 0.66101695, 0.58333333],\n",
       "       [0.5       , 0.33333333, 0.62711864, 0.45833333],\n",
       "       [0.58333333, 0.375     , 0.55932203, 0.5       ],\n",
       "       [0.63888889, 0.41666667, 0.57627119, 0.54166667],\n",
       "       [0.69444444, 0.33333333, 0.6440678 , 0.54166667],\n",
       "       [0.66666667, 0.41666667, 0.6779661 , 0.66666667],\n",
       "       [0.47222222, 0.375     , 0.59322034, 0.58333333],\n",
       "       [0.38888889, 0.25      , 0.42372881, 0.375     ],\n",
       "       [0.33333333, 0.16666667, 0.47457627, 0.41666667],\n",
       "       [0.33333333, 0.16666667, 0.45762712, 0.375     ],\n",
       "       [0.41666667, 0.29166667, 0.49152542, 0.45833333],\n",
       "       [0.47222222, 0.29166667, 0.69491525, 0.625     ],\n",
       "       [0.30555556, 0.41666667, 0.59322034, 0.58333333],\n",
       "       [0.47222222, 0.58333333, 0.59322034, 0.625     ],\n",
       "       [0.66666667, 0.45833333, 0.62711864, 0.58333333],\n",
       "       [0.55555556, 0.125     , 0.57627119, 0.5       ],\n",
       "       [0.36111111, 0.41666667, 0.52542373, 0.5       ],\n",
       "       [0.33333333, 0.20833333, 0.50847458, 0.5       ],\n",
       "       [0.33333333, 0.25      , 0.57627119, 0.45833333],\n",
       "       [0.5       , 0.41666667, 0.61016949, 0.54166667],\n",
       "       [0.41666667, 0.25      , 0.50847458, 0.45833333],\n",
       "       [0.19444444, 0.125     , 0.38983051, 0.375     ],\n",
       "       [0.36111111, 0.29166667, 0.54237288, 0.5       ],\n",
       "       [0.38888889, 0.41666667, 0.54237288, 0.45833333],\n",
       "       [0.38888889, 0.375     , 0.54237288, 0.5       ],\n",
       "       [0.52777778, 0.375     , 0.55932203, 0.5       ],\n",
       "       [0.22222222, 0.20833333, 0.33898305, 0.41666667],\n",
       "       [0.38888889, 0.33333333, 0.52542373, 0.5       ],\n",
       "       [0.55555556, 0.54166667, 0.84745763, 1.        ],\n",
       "       [0.41666667, 0.29166667, 0.69491525, 0.75      ],\n",
       "       [0.77777778, 0.41666667, 0.83050847, 0.83333333],\n",
       "       [0.55555556, 0.375     , 0.77966102, 0.70833333],\n",
       "       [0.61111111, 0.41666667, 0.81355932, 0.875     ],\n",
       "       [0.91666667, 0.41666667, 0.94915254, 0.83333333],\n",
       "       [0.16666667, 0.20833333, 0.59322034, 0.66666667],\n",
       "       [0.83333333, 0.375     , 0.89830508, 0.70833333],\n",
       "       [0.66666667, 0.20833333, 0.81355932, 0.70833333],\n",
       "       [0.80555556, 0.66666667, 0.86440678, 1.        ],\n",
       "       [0.61111111, 0.5       , 0.69491525, 0.79166667],\n",
       "       [0.58333333, 0.29166667, 0.72881356, 0.75      ],\n",
       "       [0.69444444, 0.41666667, 0.76271186, 0.83333333],\n",
       "       [0.38888889, 0.20833333, 0.6779661 , 0.79166667],\n",
       "       [0.41666667, 0.33333333, 0.69491525, 0.95833333],\n",
       "       [0.58333333, 0.5       , 0.72881356, 0.91666667],\n",
       "       [0.61111111, 0.41666667, 0.76271186, 0.70833333],\n",
       "       [0.94444444, 0.75      , 0.96610169, 0.875     ],\n",
       "       [0.94444444, 0.25      , 1.        , 0.91666667],\n",
       "       [0.47222222, 0.08333333, 0.6779661 , 0.58333333],\n",
       "       [0.72222222, 0.5       , 0.79661017, 0.91666667],\n",
       "       [0.36111111, 0.33333333, 0.66101695, 0.79166667],\n",
       "       [0.94444444, 0.33333333, 0.96610169, 0.79166667],\n",
       "       [0.55555556, 0.29166667, 0.66101695, 0.70833333],\n",
       "       [0.66666667, 0.54166667, 0.79661017, 0.83333333],\n",
       "       [0.80555556, 0.5       , 0.84745763, 0.70833333],\n",
       "       [0.52777778, 0.33333333, 0.6440678 , 0.70833333],\n",
       "       [0.5       , 0.41666667, 0.66101695, 0.70833333],\n",
       "       [0.58333333, 0.33333333, 0.77966102, 0.83333333],\n",
       "       [0.80555556, 0.41666667, 0.81355932, 0.625     ],\n",
       "       [0.86111111, 0.33333333, 0.86440678, 0.75      ],\n",
       "       [1.        , 0.75      , 0.91525424, 0.79166667],\n",
       "       [0.58333333, 0.33333333, 0.77966102, 0.875     ],\n",
       "       [0.55555556, 0.33333333, 0.69491525, 0.58333333],\n",
       "       [0.5       , 0.25      , 0.77966102, 0.54166667],\n",
       "       [0.94444444, 0.41666667, 0.86440678, 0.91666667],\n",
       "       [0.55555556, 0.58333333, 0.77966102, 0.95833333],\n",
       "       [0.58333333, 0.45833333, 0.76271186, 0.70833333],\n",
       "       [0.47222222, 0.41666667, 0.6440678 , 0.70833333],\n",
       "       [0.72222222, 0.45833333, 0.74576271, 0.83333333],\n",
       "       [0.66666667, 0.45833333, 0.77966102, 0.95833333],\n",
       "       [0.72222222, 0.45833333, 0.69491525, 0.91666667],\n",
       "       [0.41666667, 0.29166667, 0.69491525, 0.75      ],\n",
       "       [0.69444444, 0.5       , 0.83050847, 0.91666667],\n",
       "       [0.66666667, 0.54166667, 0.79661017, 1.        ],\n",
       "       [0.66666667, 0.41666667, 0.71186441, 0.91666667],\n",
       "       [0.55555556, 0.20833333, 0.6779661 , 0.75      ],\n",
       "       [0.61111111, 0.41666667, 0.71186441, 0.79166667],\n",
       "       [0.52777778, 0.58333333, 0.74576271, 0.91666667],\n",
       "       [0.44444444, 0.41666667, 0.69491525, 0.70833333]])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "92f0ac4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "x = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "d0d20b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "6b1a97e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "d85ea7e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2],\n",
       "       [5.4, 3.9, 1.7, 0.4],\n",
       "       [4.6, 3.4, 1.4, 0.3],\n",
       "       [5. , 3.4, 1.5, 0.2],\n",
       "       [4.4, 2.9, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [5.4, 3.7, 1.5, 0.2],\n",
       "       [4.8, 3.4, 1.6, 0.2],\n",
       "       [4.8, 3. , 1.4, 0.1],\n",
       "       [4.3, 3. , 1.1, 0.1],\n",
       "       [5.8, 4. , 1.2, 0.2],\n",
       "       [5.7, 4.4, 1.5, 0.4],\n",
       "       [5.4, 3.9, 1.3, 0.4],\n",
       "       [5.1, 3.5, 1.4, 0.3],\n",
       "       [5.7, 3.8, 1.7, 0.3],\n",
       "       [5.1, 3.8, 1.5, 0.3],\n",
       "       [5.4, 3.4, 1.7, 0.2],\n",
       "       [5.1, 3.7, 1.5, 0.4],\n",
       "       [4.6, 3.6, 1. , 0.2],\n",
       "       [5.1, 3.3, 1.7, 0.5],\n",
       "       [4.8, 3.4, 1.9, 0.2],\n",
       "       [5. , 3. , 1.6, 0.2],\n",
       "       [5. , 3.4, 1.6, 0.4],\n",
       "       [5.2, 3.5, 1.5, 0.2],\n",
       "       [5.2, 3.4, 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.6, 0.2],\n",
       "       [4.8, 3.1, 1.6, 0.2],\n",
       "       [5.4, 3.4, 1.5, 0.4],\n",
       "       [5.2, 4.1, 1.5, 0.1],\n",
       "       [5.5, 4.2, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.2, 1.2, 0.2],\n",
       "       [5.5, 3.5, 1.3, 0.2],\n",
       "       [4.9, 3.6, 1.4, 0.1],\n",
       "       [4.4, 3. , 1.3, 0.2],\n",
       "       [5.1, 3.4, 1.5, 0.2],\n",
       "       [5. , 3.5, 1.3, 0.3],\n",
       "       [4.5, 2.3, 1.3, 0.3],\n",
       "       [4.4, 3.2, 1.3, 0.2],\n",
       "       [5. , 3.5, 1.6, 0.6],\n",
       "       [5.1, 3.8, 1.9, 0.4],\n",
       "       [4.8, 3. , 1.4, 0.3],\n",
       "       [5.1, 3.8, 1.6, 0.2],\n",
       "       [4.6, 3.2, 1.4, 0.2],\n",
       "       [5.3, 3.7, 1.5, 0.2],\n",
       "       [5. , 3.3, 1.4, 0.2],\n",
       "       [7. , 3.2, 4.7, 1.4],\n",
       "       [6.4, 3.2, 4.5, 1.5],\n",
       "       [6.9, 3.1, 4.9, 1.5],\n",
       "       [5.5, 2.3, 4. , 1.3],\n",
       "       [6.5, 2.8, 4.6, 1.5],\n",
       "       [5.7, 2.8, 4.5, 1.3],\n",
       "       [6.3, 3.3, 4.7, 1.6],\n",
       "       [4.9, 2.4, 3.3, 1. ],\n",
       "       [6.6, 2.9, 4.6, 1.3],\n",
       "       [5.2, 2.7, 3.9, 1.4],\n",
       "       [5. , 2. , 3.5, 1. ],\n",
       "       [5.9, 3. , 4.2, 1.5],\n",
       "       [6. , 2.2, 4. , 1. ],\n",
       "       [6.1, 2.9, 4.7, 1.4],\n",
       "       [5.6, 2.9, 3.6, 1.3],\n",
       "       [6.7, 3.1, 4.4, 1.4],\n",
       "       [5.6, 3. , 4.5, 1.5],\n",
       "       [5.8, 2.7, 4.1, 1. ],\n",
       "       [6.2, 2.2, 4.5, 1.5],\n",
       "       [5.6, 2.5, 3.9, 1.1],\n",
       "       [5.9, 3.2, 4.8, 1.8],\n",
       "       [6.1, 2.8, 4. , 1.3],\n",
       "       [6.3, 2.5, 4.9, 1.5],\n",
       "       [6.1, 2.8, 4.7, 1.2],\n",
       "       [6.4, 2.9, 4.3, 1.3],\n",
       "       [6.6, 3. , 4.4, 1.4],\n",
       "       [6.8, 2.8, 4.8, 1.4],\n",
       "       [6.7, 3. , 5. , 1.7],\n",
       "       [6. , 2.9, 4.5, 1.5],\n",
       "       [5.7, 2.6, 3.5, 1. ],\n",
       "       [5.5, 2.4, 3.8, 1.1],\n",
       "       [5.5, 2.4, 3.7, 1. ],\n",
       "       [5.8, 2.7, 3.9, 1.2],\n",
       "       [6. , 2.7, 5.1, 1.6],\n",
       "       [5.4, 3. , 4.5, 1.5],\n",
       "       [6. , 3.4, 4.5, 1.6],\n",
       "       [6.7, 3.1, 4.7, 1.5],\n",
       "       [6.3, 2.3, 4.4, 1.3],\n",
       "       [5.6, 3. , 4.1, 1.3],\n",
       "       [5.5, 2.5, 4. , 1.3],\n",
       "       [5.5, 2.6, 4.4, 1.2],\n",
       "       [6.1, 3. , 4.6, 1.4],\n",
       "       [5.8, 2.6, 4. , 1.2],\n",
       "       [5. , 2.3, 3.3, 1. ],\n",
       "       [5.6, 2.7, 4.2, 1.3],\n",
       "       [5.7, 3. , 4.2, 1.2],\n",
       "       [5.7, 2.9, 4.2, 1.3],\n",
       "       [6.2, 2.9, 4.3, 1.3],\n",
       "       [5.1, 2.5, 3. , 1.1],\n",
       "       [5.7, 2.8, 4.1, 1.3],\n",
       "       [6.3, 3.3, 6. , 2.5],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [7.1, 3. , 5.9, 2.1],\n",
       "       [6.3, 2.9, 5.6, 1.8],\n",
       "       [6.5, 3. , 5.8, 2.2],\n",
       "       [7.6, 3. , 6.6, 2.1],\n",
       "       [4.9, 2.5, 4.5, 1.7],\n",
       "       [7.3, 2.9, 6.3, 1.8],\n",
       "       [6.7, 2.5, 5.8, 1.8],\n",
       "       [7.2, 3.6, 6.1, 2.5],\n",
       "       [6.5, 3.2, 5.1, 2. ],\n",
       "       [6.4, 2.7, 5.3, 1.9],\n",
       "       [6.8, 3. , 5.5, 2.1],\n",
       "       [5.7, 2.5, 5. , 2. ],\n",
       "       [5.8, 2.8, 5.1, 2.4],\n",
       "       [6.4, 3.2, 5.3, 2.3],\n",
       "       [6.5, 3. , 5.5, 1.8],\n",
       "       [7.7, 3.8, 6.7, 2.2],\n",
       "       [7.7, 2.6, 6.9, 2.3],\n",
       "       [6. , 2.2, 5. , 1.5],\n",
       "       [6.9, 3.2, 5.7, 2.3],\n",
       "       [5.6, 2.8, 4.9, 2. ],\n",
       "       [7.7, 2.8, 6.7, 2. ],\n",
       "       [6.3, 2.7, 4.9, 1.8],\n",
       "       [6.7, 3.3, 5.7, 2.1],\n",
       "       [7.2, 3.2, 6. , 1.8],\n",
       "       [6.2, 2.8, 4.8, 1.8],\n",
       "       [6.1, 3. , 4.9, 1.8],\n",
       "       [6.4, 2.8, 5.6, 2.1],\n",
       "       [7.2, 3. , 5.8, 1.6],\n",
       "       [7.4, 2.8, 6.1, 1.9],\n",
       "       [7.9, 3.8, 6.4, 2. ],\n",
       "       [6.4, 2.8, 5.6, 2.2],\n",
       "       [6.3, 2.8, 5.1, 1.5],\n",
       "       [6.1, 2.6, 5.6, 1.4],\n",
       "       [7.7, 3. , 6.1, 2.3],\n",
       "       [6.3, 3.4, 5.6, 2.4],\n",
       "       [6.4, 3.1, 5.5, 1.8],\n",
       "       [6. , 3. , 4.8, 1.8],\n",
       "       [6.9, 3.1, 5.4, 2.1],\n",
       "       [6.7, 3.1, 5.6, 2.4],\n",
       "       [6.9, 3.1, 5.1, 2.3],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [6.8, 3.2, 5.9, 2.3],\n",
       "       [6.7, 3.3, 5.7, 2.5],\n",
       "       [6.7, 3. , 5.2, 2.3],\n",
       "       [6.3, 2.5, 5. , 1.9],\n",
       "       [6.5, 3. , 5.2, 2. ],\n",
       "       [6.2, 3.4, 5.4, 2.3],\n",
       "       [5.9, 3. , 5.1, 1.8]])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "6b411d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long) # long long int -> bentuk int lebih gede\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "01b50d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(4, 8),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(8, 3),\n",
    "    # nn.Softmax() \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "b574b438",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fun = nn.CrossEntropyLoss() # include Softmax()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7939faaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ecd25879",
   "metadata": {},
   "source": [
    "### Feed Forward -> Compute Loss -> Back Propagation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "083eaaaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n"
     ]
    }
   ],
   "source": [
    "print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "627be48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-1/50. Loss: 1.1423. Test Acc: 33.3333%\n",
      "Epoch-2/50. Loss: 1.1975. Test Acc: 33.3333%\n",
      "Epoch-3/50. Loss: 1.1123. Test Acc: 33.3333%\n",
      "Epoch-4/50. Loss: 1.0979. Test Acc: 33.3333%\n",
      "Epoch-5/50. Loss: 1.0845. Test Acc: 33.3333%\n",
      "Epoch-6/50. Loss: 1.0703. Test Acc: 33.3333%\n",
      "Epoch-7/50. Loss: 1.0448. Test Acc: 33.3333%\n",
      "Epoch-8/50. Loss: 1.0027. Test Acc: 36.6667%\n",
      "Epoch-9/50. Loss: 0.9398. Test Acc: 66.6667%\n",
      "Epoch-10/50. Loss: 0.8811. Test Acc: 66.6667%\n",
      "Epoch-11/50. Loss: 0.8190. Test Acc: 66.6667%\n",
      "Epoch-12/50. Loss: 0.7673. Test Acc: 66.6667%\n",
      "Epoch-13/50. Loss: 0.7309. Test Acc: 66.6667%\n",
      "Epoch-14/50. Loss: 0.6803. Test Acc: 76.6667%\n",
      "Epoch-15/50. Loss: 0.6454. Test Acc: 96.6667%\n",
      "Epoch-16/50. Loss: 0.6251. Test Acc: 96.6667%\n",
      "Epoch-17/50. Loss: 0.5938. Test Acc: 76.6667%\n",
      "Epoch-18/50. Loss: 0.5595. Test Acc: 73.3333%\n",
      "Epoch-19/50. Loss: 0.5412. Test Acc: 73.3333%\n",
      "Epoch-20/50. Loss: 0.5226. Test Acc: 76.6667%\n",
      "Epoch-21/50. Loss: 0.4947. Test Acc: 86.6667%\n",
      "Epoch-22/50. Loss: 0.4768. Test Acc: 96.6667%\n",
      "Epoch-23/50. Loss: 0.4643. Test Acc: 96.6667%\n",
      "Epoch-24/50. Loss: 0.4446. Test Acc: 80.0000%\n",
      "Epoch-25/50. Loss: 0.4260. Test Acc: 76.6667%\n",
      "Epoch-26/50. Loss: 0.4146. Test Acc: 76.6667%\n",
      "Epoch-27/50. Loss: 0.4015. Test Acc: 86.6667%\n",
      "Epoch-28/50. Loss: 0.3871. Test Acc: 96.6667%\n",
      "Epoch-29/50. Loss: 0.3786. Test Acc: 96.6667%\n",
      "Epoch-30/50. Loss: 0.3686. Test Acc: 90.0000%\n",
      "Epoch-31/50. Loss: 0.3545. Test Acc: 86.6667%\n",
      "Epoch-32/50. Loss: 0.3454. Test Acc: 86.6667%\n",
      "Epoch-33/50. Loss: 0.3353. Test Acc: 96.6667%\n",
      "Epoch-34/50. Loss: 0.3225. Test Acc: 96.6667%\n",
      "Epoch-35/50. Loss: 0.3141. Test Acc: 96.6667%\n",
      "Epoch-36/50. Loss: 0.3051. Test Acc: 96.6667%\n",
      "Epoch-37/50. Loss: 0.2944. Test Acc: 96.6667%\n",
      "Epoch-38/50. Loss: 0.2871. Test Acc: 96.6667%\n",
      "Epoch-39/50. Loss: 0.2787. Test Acc: 96.6667%\n",
      "Epoch-40/50. Loss: 0.2694. Test Acc: 96.6667%\n",
      "Epoch-41/50. Loss: 0.2628. Test Acc: 96.6667%\n",
      "Epoch-42/50. Loss: 0.2548. Test Acc: 96.6667%\n",
      "Epoch-43/50. Loss: 0.2472. Test Acc: 96.6667%\n",
      "Epoch-44/50. Loss: 0.2415. Test Acc: 96.6667%\n",
      "Epoch-45/50. Loss: 0.2344. Test Acc: 96.6667%\n",
      "Epoch-46/50. Loss: 0.2291. Test Acc: 96.6667%\n",
      "Epoch-47/50. Loss: 0.2237. Test Acc: 96.6667%\n",
      "Epoch-48/50. Loss: 0.2180. Test Acc: 96.6667%\n",
      "Epoch-49/50. Loss: 0.2138. Test Acc: 96.6667%\n",
      "Epoch-50/50. Loss: 0.2085. Test Acc: 96.6667%\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad() # gradient descent ke reset\n",
    "    outputs = model(X_train)\n",
    "    loss = loss_fun(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step() # menjalankan gradient descent\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(X_test)\n",
    "        _, predicted = torch.max(test_outputs, 1) # _ = abaikan\n",
    "        acc = (predicted == y_test).sum() / len(y_test)\n",
    "\n",
    "    print(f'Epoch-{epoch + 1}/{epochs}. Loss: {loss.item():.4f}. Test Acc: {acc*100:.4f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b6bb0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0592892b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c18260",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7879b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53babd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf799a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22d19cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a9247f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9189c3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac6021b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47be31c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a2d054",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ba2d1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "4dac6dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "10c59b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "ea8dd050",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "c38dc5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "5574e2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(4, 8),   # x * W + B  # input 4 -> hidden 8\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(8, 3)    # hidden 8 -> output 3 (kelas)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "6aa2bd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fun = nn.CrossEntropyLoss() # include softmax\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)  # Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "0a34cea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500, Loss: 1.0914, Test Acc: 33.33%\n",
      "Epoch 2/500, Loss: 1.0869, Test Acc: 33.33%\n",
      "Epoch 3/500, Loss: 1.0824, Test Acc: 33.33%\n",
      "Epoch 4/500, Loss: 1.0780, Test Acc: 33.33%\n",
      "Epoch 5/500, Loss: 1.0737, Test Acc: 33.33%\n",
      "Epoch 6/500, Loss: 1.0695, Test Acc: 33.33%\n",
      "Epoch 7/500, Loss: 1.0655, Test Acc: 33.33%\n",
      "Epoch 8/500, Loss: 1.0615, Test Acc: 33.33%\n",
      "Epoch 9/500, Loss: 1.0575, Test Acc: 33.33%\n",
      "Epoch 10/500, Loss: 1.0535, Test Acc: 33.33%\n",
      "Epoch 11/500, Loss: 1.0495, Test Acc: 33.33%\n",
      "Epoch 12/500, Loss: 1.0456, Test Acc: 36.67%\n",
      "Epoch 13/500, Loss: 1.0417, Test Acc: 40.00%\n",
      "Epoch 14/500, Loss: 1.0378, Test Acc: 43.33%\n",
      "Epoch 15/500, Loss: 1.0338, Test Acc: 43.33%\n",
      "Epoch 16/500, Loss: 1.0299, Test Acc: 43.33%\n",
      "Epoch 17/500, Loss: 1.0259, Test Acc: 50.00%\n",
      "Epoch 18/500, Loss: 1.0218, Test Acc: 53.33%\n",
      "Epoch 19/500, Loss: 1.0178, Test Acc: 63.33%\n",
      "Epoch 20/500, Loss: 1.0137, Test Acc: 70.00%\n",
      "Epoch 21/500, Loss: 1.0095, Test Acc: 73.33%\n",
      "Epoch 22/500, Loss: 1.0053, Test Acc: 70.00%\n",
      "Epoch 23/500, Loss: 1.0010, Test Acc: 70.00%\n",
      "Epoch 24/500, Loss: 0.9967, Test Acc: 70.00%\n",
      "Epoch 25/500, Loss: 0.9923, Test Acc: 73.33%\n",
      "Epoch 26/500, Loss: 0.9879, Test Acc: 76.67%\n",
      "Epoch 27/500, Loss: 0.9833, Test Acc: 76.67%\n",
      "Epoch 28/500, Loss: 0.9787, Test Acc: 76.67%\n",
      "Epoch 29/500, Loss: 0.9741, Test Acc: 76.67%\n",
      "Epoch 30/500, Loss: 0.9693, Test Acc: 76.67%\n",
      "Epoch 31/500, Loss: 0.9644, Test Acc: 76.67%\n",
      "Epoch 32/500, Loss: 0.9595, Test Acc: 76.67%\n",
      "Epoch 33/500, Loss: 0.9545, Test Acc: 76.67%\n",
      "Epoch 34/500, Loss: 0.9494, Test Acc: 76.67%\n",
      "Epoch 35/500, Loss: 0.9443, Test Acc: 76.67%\n",
      "Epoch 36/500, Loss: 0.9390, Test Acc: 76.67%\n",
      "Epoch 37/500, Loss: 0.9337, Test Acc: 76.67%\n",
      "Epoch 38/500, Loss: 0.9283, Test Acc: 76.67%\n",
      "Epoch 39/500, Loss: 0.9229, Test Acc: 76.67%\n",
      "Epoch 40/500, Loss: 0.9174, Test Acc: 76.67%\n",
      "Epoch 41/500, Loss: 0.9118, Test Acc: 76.67%\n",
      "Epoch 42/500, Loss: 0.9061, Test Acc: 76.67%\n",
      "Epoch 43/500, Loss: 0.9005, Test Acc: 73.33%\n",
      "Epoch 44/500, Loss: 0.8947, Test Acc: 73.33%\n",
      "Epoch 45/500, Loss: 0.8890, Test Acc: 73.33%\n",
      "Epoch 46/500, Loss: 0.8831, Test Acc: 73.33%\n",
      "Epoch 47/500, Loss: 0.8773, Test Acc: 73.33%\n",
      "Epoch 48/500, Loss: 0.8714, Test Acc: 73.33%\n",
      "Epoch 49/500, Loss: 0.8654, Test Acc: 73.33%\n",
      "Epoch 50/500, Loss: 0.8594, Test Acc: 73.33%\n",
      "Epoch 51/500, Loss: 0.8534, Test Acc: 73.33%\n",
      "Epoch 52/500, Loss: 0.8474, Test Acc: 73.33%\n",
      "Epoch 53/500, Loss: 0.8413, Test Acc: 73.33%\n",
      "Epoch 54/500, Loss: 0.8353, Test Acc: 73.33%\n",
      "Epoch 55/500, Loss: 0.8292, Test Acc: 73.33%\n",
      "Epoch 56/500, Loss: 0.8231, Test Acc: 73.33%\n",
      "Epoch 57/500, Loss: 0.8169, Test Acc: 73.33%\n",
      "Epoch 58/500, Loss: 0.8108, Test Acc: 73.33%\n",
      "Epoch 59/500, Loss: 0.8047, Test Acc: 73.33%\n",
      "Epoch 60/500, Loss: 0.7986, Test Acc: 73.33%\n",
      "Epoch 61/500, Loss: 0.7924, Test Acc: 73.33%\n",
      "Epoch 62/500, Loss: 0.7863, Test Acc: 73.33%\n",
      "Epoch 63/500, Loss: 0.7802, Test Acc: 73.33%\n",
      "Epoch 64/500, Loss: 0.7742, Test Acc: 73.33%\n",
      "Epoch 65/500, Loss: 0.7681, Test Acc: 73.33%\n",
      "Epoch 66/500, Loss: 0.7621, Test Acc: 73.33%\n",
      "Epoch 67/500, Loss: 0.7561, Test Acc: 76.67%\n",
      "Epoch 68/500, Loss: 0.7502, Test Acc: 76.67%\n",
      "Epoch 69/500, Loss: 0.7443, Test Acc: 76.67%\n",
      "Epoch 70/500, Loss: 0.7384, Test Acc: 76.67%\n",
      "Epoch 71/500, Loss: 0.7326, Test Acc: 76.67%\n",
      "Epoch 72/500, Loss: 0.7268, Test Acc: 76.67%\n",
      "Epoch 73/500, Loss: 0.7210, Test Acc: 76.67%\n",
      "Epoch 74/500, Loss: 0.7153, Test Acc: 76.67%\n",
      "Epoch 75/500, Loss: 0.7097, Test Acc: 76.67%\n",
      "Epoch 76/500, Loss: 0.7041, Test Acc: 76.67%\n",
      "Epoch 77/500, Loss: 0.6985, Test Acc: 76.67%\n",
      "Epoch 78/500, Loss: 0.6930, Test Acc: 76.67%\n",
      "Epoch 79/500, Loss: 0.6876, Test Acc: 76.67%\n",
      "Epoch 80/500, Loss: 0.6823, Test Acc: 76.67%\n",
      "Epoch 81/500, Loss: 0.6770, Test Acc: 76.67%\n",
      "Epoch 82/500, Loss: 0.6718, Test Acc: 76.67%\n",
      "Epoch 83/500, Loss: 0.6666, Test Acc: 76.67%\n",
      "Epoch 84/500, Loss: 0.6615, Test Acc: 76.67%\n",
      "Epoch 85/500, Loss: 0.6564, Test Acc: 76.67%\n",
      "Epoch 86/500, Loss: 0.6514, Test Acc: 76.67%\n",
      "Epoch 87/500, Loss: 0.6465, Test Acc: 76.67%\n",
      "Epoch 88/500, Loss: 0.6416, Test Acc: 76.67%\n",
      "Epoch 89/500, Loss: 0.6368, Test Acc: 76.67%\n",
      "Epoch 90/500, Loss: 0.6321, Test Acc: 76.67%\n",
      "Epoch 91/500, Loss: 0.6274, Test Acc: 76.67%\n",
      "Epoch 92/500, Loss: 0.6228, Test Acc: 76.67%\n",
      "Epoch 93/500, Loss: 0.6182, Test Acc: 76.67%\n",
      "Epoch 94/500, Loss: 0.6138, Test Acc: 76.67%\n",
      "Epoch 95/500, Loss: 0.6094, Test Acc: 76.67%\n",
      "Epoch 96/500, Loss: 0.6050, Test Acc: 76.67%\n",
      "Epoch 97/500, Loss: 0.6007, Test Acc: 76.67%\n",
      "Epoch 98/500, Loss: 0.5965, Test Acc: 76.67%\n",
      "Epoch 99/500, Loss: 0.5924, Test Acc: 76.67%\n",
      "Epoch 100/500, Loss: 0.5883, Test Acc: 76.67%\n",
      "Epoch 101/500, Loss: 0.5843, Test Acc: 76.67%\n",
      "Epoch 102/500, Loss: 0.5803, Test Acc: 76.67%\n",
      "Epoch 103/500, Loss: 0.5764, Test Acc: 76.67%\n",
      "Epoch 104/500, Loss: 0.5726, Test Acc: 76.67%\n",
      "Epoch 105/500, Loss: 0.5688, Test Acc: 76.67%\n",
      "Epoch 106/500, Loss: 0.5651, Test Acc: 76.67%\n",
      "Epoch 107/500, Loss: 0.5614, Test Acc: 76.67%\n",
      "Epoch 108/500, Loss: 0.5578, Test Acc: 76.67%\n",
      "Epoch 109/500, Loss: 0.5542, Test Acc: 76.67%\n",
      "Epoch 110/500, Loss: 0.5507, Test Acc: 76.67%\n",
      "Epoch 111/500, Loss: 0.5473, Test Acc: 76.67%\n",
      "Epoch 112/500, Loss: 0.5439, Test Acc: 76.67%\n",
      "Epoch 113/500, Loss: 0.5406, Test Acc: 76.67%\n",
      "Epoch 114/500, Loss: 0.5373, Test Acc: 76.67%\n",
      "Epoch 115/500, Loss: 0.5341, Test Acc: 76.67%\n",
      "Epoch 116/500, Loss: 0.5309, Test Acc: 76.67%\n",
      "Epoch 117/500, Loss: 0.5278, Test Acc: 76.67%\n",
      "Epoch 118/500, Loss: 0.5247, Test Acc: 76.67%\n",
      "Epoch 119/500, Loss: 0.5216, Test Acc: 76.67%\n",
      "Epoch 120/500, Loss: 0.5186, Test Acc: 80.00%\n",
      "Epoch 121/500, Loss: 0.5157, Test Acc: 80.00%\n",
      "Epoch 122/500, Loss: 0.5128, Test Acc: 80.00%\n",
      "Epoch 123/500, Loss: 0.5099, Test Acc: 80.00%\n",
      "Epoch 124/500, Loss: 0.5071, Test Acc: 80.00%\n",
      "Epoch 125/500, Loss: 0.5043, Test Acc: 80.00%\n",
      "Epoch 126/500, Loss: 0.5015, Test Acc: 80.00%\n",
      "Epoch 127/500, Loss: 0.4988, Test Acc: 80.00%\n",
      "Epoch 128/500, Loss: 0.4962, Test Acc: 80.00%\n",
      "Epoch 129/500, Loss: 0.4935, Test Acc: 80.00%\n",
      "Epoch 130/500, Loss: 0.4909, Test Acc: 80.00%\n",
      "Epoch 131/500, Loss: 0.4884, Test Acc: 80.00%\n",
      "Epoch 132/500, Loss: 0.4858, Test Acc: 80.00%\n",
      "Epoch 133/500, Loss: 0.4833, Test Acc: 80.00%\n",
      "Epoch 134/500, Loss: 0.4809, Test Acc: 80.00%\n",
      "Epoch 135/500, Loss: 0.4784, Test Acc: 80.00%\n",
      "Epoch 136/500, Loss: 0.4760, Test Acc: 80.00%\n",
      "Epoch 137/500, Loss: 0.4737, Test Acc: 80.00%\n",
      "Epoch 138/500, Loss: 0.4713, Test Acc: 80.00%\n",
      "Epoch 139/500, Loss: 0.4690, Test Acc: 80.00%\n",
      "Epoch 140/500, Loss: 0.4667, Test Acc: 80.00%\n",
      "Epoch 141/500, Loss: 0.4645, Test Acc: 80.00%\n",
      "Epoch 142/500, Loss: 0.4622, Test Acc: 80.00%\n",
      "Epoch 143/500, Loss: 0.4600, Test Acc: 80.00%\n",
      "Epoch 144/500, Loss: 0.4579, Test Acc: 80.00%\n",
      "Epoch 145/500, Loss: 0.4557, Test Acc: 80.00%\n",
      "Epoch 146/500, Loss: 0.4536, Test Acc: 80.00%\n",
      "Epoch 147/500, Loss: 0.4515, Test Acc: 80.00%\n",
      "Epoch 148/500, Loss: 0.4494, Test Acc: 80.00%\n",
      "Epoch 149/500, Loss: 0.4473, Test Acc: 86.67%\n",
      "Epoch 150/500, Loss: 0.4453, Test Acc: 90.00%\n",
      "Epoch 151/500, Loss: 0.4433, Test Acc: 90.00%\n",
      "Epoch 152/500, Loss: 0.4413, Test Acc: 90.00%\n",
      "Epoch 153/500, Loss: 0.4393, Test Acc: 90.00%\n",
      "Epoch 154/500, Loss: 0.4374, Test Acc: 90.00%\n",
      "Epoch 155/500, Loss: 0.4354, Test Acc: 90.00%\n",
      "Epoch 156/500, Loss: 0.4335, Test Acc: 90.00%\n",
      "Epoch 157/500, Loss: 0.4316, Test Acc: 93.33%\n",
      "Epoch 158/500, Loss: 0.4298, Test Acc: 93.33%\n",
      "Epoch 159/500, Loss: 0.4279, Test Acc: 93.33%\n",
      "Epoch 160/500, Loss: 0.4261, Test Acc: 93.33%\n",
      "Epoch 161/500, Loss: 0.4243, Test Acc: 93.33%\n",
      "Epoch 162/500, Loss: 0.4224, Test Acc: 93.33%\n",
      "Epoch 163/500, Loss: 0.4207, Test Acc: 93.33%\n",
      "Epoch 164/500, Loss: 0.4189, Test Acc: 93.33%\n",
      "Epoch 165/500, Loss: 0.4171, Test Acc: 93.33%\n",
      "Epoch 166/500, Loss: 0.4154, Test Acc: 93.33%\n",
      "Epoch 167/500, Loss: 0.4137, Test Acc: 93.33%\n",
      "Epoch 168/500, Loss: 0.4120, Test Acc: 93.33%\n",
      "Epoch 169/500, Loss: 0.4103, Test Acc: 93.33%\n",
      "Epoch 170/500, Loss: 0.4086, Test Acc: 93.33%\n",
      "Epoch 171/500, Loss: 0.4069, Test Acc: 93.33%\n",
      "Epoch 172/500, Loss: 0.4052, Test Acc: 93.33%\n",
      "Epoch 173/500, Loss: 0.4036, Test Acc: 93.33%\n",
      "Epoch 174/500, Loss: 0.4020, Test Acc: 93.33%\n",
      "Epoch 175/500, Loss: 0.4003, Test Acc: 93.33%\n",
      "Epoch 176/500, Loss: 0.3987, Test Acc: 93.33%\n",
      "Epoch 177/500, Loss: 0.3971, Test Acc: 93.33%\n",
      "Epoch 178/500, Loss: 0.3955, Test Acc: 93.33%\n",
      "Epoch 179/500, Loss: 0.3940, Test Acc: 93.33%\n",
      "Epoch 180/500, Loss: 0.3924, Test Acc: 93.33%\n",
      "Epoch 181/500, Loss: 0.3909, Test Acc: 93.33%\n",
      "Epoch 182/500, Loss: 0.3893, Test Acc: 93.33%\n",
      "Epoch 183/500, Loss: 0.3878, Test Acc: 93.33%\n",
      "Epoch 184/500, Loss: 0.3863, Test Acc: 96.67%\n",
      "Epoch 185/500, Loss: 0.3848, Test Acc: 96.67%\n",
      "Epoch 186/500, Loss: 0.3833, Test Acc: 96.67%\n",
      "Epoch 187/500, Loss: 0.3818, Test Acc: 96.67%\n",
      "Epoch 188/500, Loss: 0.3803, Test Acc: 96.67%\n",
      "Epoch 189/500, Loss: 0.3788, Test Acc: 96.67%\n",
      "Epoch 190/500, Loss: 0.3773, Test Acc: 96.67%\n",
      "Epoch 191/500, Loss: 0.3759, Test Acc: 96.67%\n",
      "Epoch 192/500, Loss: 0.3744, Test Acc: 96.67%\n",
      "Epoch 193/500, Loss: 0.3730, Test Acc: 96.67%\n",
      "Epoch 194/500, Loss: 0.3716, Test Acc: 96.67%\n",
      "Epoch 195/500, Loss: 0.3702, Test Acc: 96.67%\n",
      "Epoch 196/500, Loss: 0.3687, Test Acc: 96.67%\n",
      "Epoch 197/500, Loss: 0.3673, Test Acc: 96.67%\n",
      "Epoch 198/500, Loss: 0.3659, Test Acc: 96.67%\n",
      "Epoch 199/500, Loss: 0.3645, Test Acc: 96.67%\n",
      "Epoch 200/500, Loss: 0.3632, Test Acc: 96.67%\n",
      "Epoch 201/500, Loss: 0.3618, Test Acc: 96.67%\n",
      "Epoch 202/500, Loss: 0.3604, Test Acc: 96.67%\n",
      "Epoch 203/500, Loss: 0.3591, Test Acc: 96.67%\n",
      "Epoch 204/500, Loss: 0.3577, Test Acc: 96.67%\n",
      "Epoch 205/500, Loss: 0.3564, Test Acc: 96.67%\n",
      "Epoch 206/500, Loss: 0.3550, Test Acc: 96.67%\n",
      "Epoch 207/500, Loss: 0.3537, Test Acc: 96.67%\n",
      "Epoch 208/500, Loss: 0.3524, Test Acc: 96.67%\n",
      "Epoch 209/500, Loss: 0.3511, Test Acc: 96.67%\n",
      "Epoch 210/500, Loss: 0.3497, Test Acc: 96.67%\n",
      "Epoch 211/500, Loss: 0.3484, Test Acc: 96.67%\n",
      "Epoch 212/500, Loss: 0.3471, Test Acc: 96.67%\n",
      "Epoch 213/500, Loss: 0.3459, Test Acc: 96.67%\n",
      "Epoch 214/500, Loss: 0.3446, Test Acc: 96.67%\n",
      "Epoch 215/500, Loss: 0.3433, Test Acc: 96.67%\n",
      "Epoch 216/500, Loss: 0.3420, Test Acc: 96.67%\n",
      "Epoch 217/500, Loss: 0.3408, Test Acc: 96.67%\n",
      "Epoch 218/500, Loss: 0.3395, Test Acc: 96.67%\n",
      "Epoch 219/500, Loss: 0.3382, Test Acc: 96.67%\n",
      "Epoch 220/500, Loss: 0.3370, Test Acc: 96.67%\n",
      "Epoch 221/500, Loss: 0.3358, Test Acc: 96.67%\n",
      "Epoch 222/500, Loss: 0.3345, Test Acc: 96.67%\n",
      "Epoch 223/500, Loss: 0.3333, Test Acc: 96.67%\n",
      "Epoch 224/500, Loss: 0.3321, Test Acc: 96.67%\n",
      "Epoch 225/500, Loss: 0.3309, Test Acc: 96.67%\n",
      "Epoch 226/500, Loss: 0.3297, Test Acc: 96.67%\n",
      "Epoch 227/500, Loss: 0.3285, Test Acc: 96.67%\n",
      "Epoch 228/500, Loss: 0.3273, Test Acc: 96.67%\n",
      "Epoch 229/500, Loss: 0.3261, Test Acc: 96.67%\n",
      "Epoch 230/500, Loss: 0.3249, Test Acc: 96.67%\n",
      "Epoch 231/500, Loss: 0.3237, Test Acc: 96.67%\n",
      "Epoch 232/500, Loss: 0.3225, Test Acc: 96.67%\n",
      "Epoch 233/500, Loss: 0.3214, Test Acc: 96.67%\n",
      "Epoch 234/500, Loss: 0.3202, Test Acc: 96.67%\n",
      "Epoch 235/500, Loss: 0.3190, Test Acc: 96.67%\n",
      "Epoch 236/500, Loss: 0.3179, Test Acc: 96.67%\n",
      "Epoch 237/500, Loss: 0.3167, Test Acc: 96.67%\n",
      "Epoch 238/500, Loss: 0.3156, Test Acc: 96.67%\n",
      "Epoch 239/500, Loss: 0.3145, Test Acc: 96.67%\n",
      "Epoch 240/500, Loss: 0.3133, Test Acc: 96.67%\n",
      "Epoch 241/500, Loss: 0.3122, Test Acc: 96.67%\n",
      "Epoch 242/500, Loss: 0.3111, Test Acc: 96.67%\n",
      "Epoch 243/500, Loss: 0.3100, Test Acc: 96.67%\n",
      "Epoch 244/500, Loss: 0.3089, Test Acc: 96.67%\n",
      "Epoch 245/500, Loss: 0.3078, Test Acc: 96.67%\n",
      "Epoch 246/500, Loss: 0.3067, Test Acc: 96.67%\n",
      "Epoch 247/500, Loss: 0.3056, Test Acc: 96.67%\n",
      "Epoch 248/500, Loss: 0.3045, Test Acc: 96.67%\n",
      "Epoch 249/500, Loss: 0.3034, Test Acc: 96.67%\n",
      "Epoch 250/500, Loss: 0.3024, Test Acc: 96.67%\n",
      "Epoch 251/500, Loss: 0.3013, Test Acc: 96.67%\n",
      "Epoch 252/500, Loss: 0.3002, Test Acc: 96.67%\n",
      "Epoch 253/500, Loss: 0.2992, Test Acc: 96.67%\n",
      "Epoch 254/500, Loss: 0.2981, Test Acc: 96.67%\n",
      "Epoch 255/500, Loss: 0.2971, Test Acc: 96.67%\n",
      "Epoch 256/500, Loss: 0.2960, Test Acc: 96.67%\n",
      "Epoch 257/500, Loss: 0.2950, Test Acc: 96.67%\n",
      "Epoch 258/500, Loss: 0.2939, Test Acc: 96.67%\n",
      "Epoch 259/500, Loss: 0.2929, Test Acc: 96.67%\n",
      "Epoch 260/500, Loss: 0.2919, Test Acc: 96.67%\n",
      "Epoch 261/500, Loss: 0.2908, Test Acc: 96.67%\n",
      "Epoch 262/500, Loss: 0.2898, Test Acc: 96.67%\n",
      "Epoch 263/500, Loss: 0.2888, Test Acc: 96.67%\n",
      "Epoch 264/500, Loss: 0.2878, Test Acc: 96.67%\n",
      "Epoch 265/500, Loss: 0.2868, Test Acc: 96.67%\n",
      "Epoch 266/500, Loss: 0.2858, Test Acc: 96.67%\n",
      "Epoch 267/500, Loss: 0.2848, Test Acc: 96.67%\n",
      "Epoch 268/500, Loss: 0.2838, Test Acc: 96.67%\n",
      "Epoch 269/500, Loss: 0.2829, Test Acc: 96.67%\n",
      "Epoch 270/500, Loss: 0.2819, Test Acc: 96.67%\n",
      "Epoch 271/500, Loss: 0.2809, Test Acc: 96.67%\n",
      "Epoch 272/500, Loss: 0.2799, Test Acc: 96.67%\n",
      "Epoch 273/500, Loss: 0.2790, Test Acc: 96.67%\n",
      "Epoch 274/500, Loss: 0.2780, Test Acc: 96.67%\n",
      "Epoch 275/500, Loss: 0.2771, Test Acc: 96.67%\n",
      "Epoch 276/500, Loss: 0.2761, Test Acc: 96.67%\n",
      "Epoch 277/500, Loss: 0.2752, Test Acc: 96.67%\n",
      "Epoch 278/500, Loss: 0.2742, Test Acc: 96.67%\n",
      "Epoch 279/500, Loss: 0.2733, Test Acc: 96.67%\n",
      "Epoch 280/500, Loss: 0.2724, Test Acc: 96.67%\n",
      "Epoch 281/500, Loss: 0.2714, Test Acc: 96.67%\n",
      "Epoch 282/500, Loss: 0.2705, Test Acc: 96.67%\n",
      "Epoch 283/500, Loss: 0.2696, Test Acc: 96.67%\n",
      "Epoch 284/500, Loss: 0.2687, Test Acc: 96.67%\n",
      "Epoch 285/500, Loss: 0.2678, Test Acc: 96.67%\n",
      "Epoch 286/500, Loss: 0.2669, Test Acc: 96.67%\n",
      "Epoch 287/500, Loss: 0.2660, Test Acc: 96.67%\n",
      "Epoch 288/500, Loss: 0.2651, Test Acc: 96.67%\n",
      "Epoch 289/500, Loss: 0.2642, Test Acc: 96.67%\n",
      "Epoch 290/500, Loss: 0.2633, Test Acc: 96.67%\n",
      "Epoch 291/500, Loss: 0.2624, Test Acc: 96.67%\n",
      "Epoch 292/500, Loss: 0.2615, Test Acc: 96.67%\n",
      "Epoch 293/500, Loss: 0.2606, Test Acc: 96.67%\n",
      "Epoch 294/500, Loss: 0.2598, Test Acc: 96.67%\n",
      "Epoch 295/500, Loss: 0.2589, Test Acc: 96.67%\n",
      "Epoch 296/500, Loss: 0.2580, Test Acc: 96.67%\n",
      "Epoch 297/500, Loss: 0.2572, Test Acc: 96.67%\n",
      "Epoch 298/500, Loss: 0.2563, Test Acc: 96.67%\n",
      "Epoch 299/500, Loss: 0.2555, Test Acc: 96.67%\n",
      "Epoch 300/500, Loss: 0.2547, Test Acc: 96.67%\n",
      "Epoch 301/500, Loss: 0.2538, Test Acc: 96.67%\n",
      "Epoch 302/500, Loss: 0.2530, Test Acc: 96.67%\n",
      "Epoch 303/500, Loss: 0.2522, Test Acc: 96.67%\n",
      "Epoch 304/500, Loss: 0.2513, Test Acc: 96.67%\n",
      "Epoch 305/500, Loss: 0.2505, Test Acc: 96.67%\n",
      "Epoch 306/500, Loss: 0.2497, Test Acc: 96.67%\n",
      "Epoch 307/500, Loss: 0.2489, Test Acc: 96.67%\n",
      "Epoch 308/500, Loss: 0.2481, Test Acc: 96.67%\n",
      "Epoch 309/500, Loss: 0.2473, Test Acc: 96.67%\n",
      "Epoch 310/500, Loss: 0.2465, Test Acc: 96.67%\n",
      "Epoch 311/500, Loss: 0.2457, Test Acc: 96.67%\n",
      "Epoch 312/500, Loss: 0.2449, Test Acc: 96.67%\n",
      "Epoch 313/500, Loss: 0.2441, Test Acc: 96.67%\n",
      "Epoch 314/500, Loss: 0.2433, Test Acc: 96.67%\n",
      "Epoch 315/500, Loss: 0.2425, Test Acc: 96.67%\n",
      "Epoch 316/500, Loss: 0.2417, Test Acc: 96.67%\n",
      "Epoch 317/500, Loss: 0.2409, Test Acc: 96.67%\n",
      "Epoch 318/500, Loss: 0.2402, Test Acc: 96.67%\n",
      "Epoch 319/500, Loss: 0.2394, Test Acc: 96.67%\n",
      "Epoch 320/500, Loss: 0.2386, Test Acc: 96.67%\n",
      "Epoch 321/500, Loss: 0.2379, Test Acc: 96.67%\n",
      "Epoch 322/500, Loss: 0.2371, Test Acc: 96.67%\n",
      "Epoch 323/500, Loss: 0.2364, Test Acc: 96.67%\n",
      "Epoch 324/500, Loss: 0.2356, Test Acc: 96.67%\n",
      "Epoch 325/500, Loss: 0.2349, Test Acc: 96.67%\n",
      "Epoch 326/500, Loss: 0.2341, Test Acc: 96.67%\n",
      "Epoch 327/500, Loss: 0.2334, Test Acc: 96.67%\n",
      "Epoch 328/500, Loss: 0.2327, Test Acc: 96.67%\n",
      "Epoch 329/500, Loss: 0.2319, Test Acc: 96.67%\n",
      "Epoch 330/500, Loss: 0.2312, Test Acc: 96.67%\n",
      "Epoch 331/500, Loss: 0.2305, Test Acc: 96.67%\n",
      "Epoch 332/500, Loss: 0.2298, Test Acc: 96.67%\n",
      "Epoch 333/500, Loss: 0.2291, Test Acc: 96.67%\n",
      "Epoch 334/500, Loss: 0.2283, Test Acc: 96.67%\n",
      "Epoch 335/500, Loss: 0.2276, Test Acc: 96.67%\n",
      "Epoch 336/500, Loss: 0.2269, Test Acc: 96.67%\n",
      "Epoch 337/500, Loss: 0.2262, Test Acc: 96.67%\n",
      "Epoch 338/500, Loss: 0.2255, Test Acc: 96.67%\n",
      "Epoch 339/500, Loss: 0.2248, Test Acc: 96.67%\n",
      "Epoch 340/500, Loss: 0.2242, Test Acc: 96.67%\n",
      "Epoch 341/500, Loss: 0.2235, Test Acc: 96.67%\n",
      "Epoch 342/500, Loss: 0.2228, Test Acc: 96.67%\n",
      "Epoch 343/500, Loss: 0.2221, Test Acc: 96.67%\n",
      "Epoch 344/500, Loss: 0.2214, Test Acc: 96.67%\n",
      "Epoch 345/500, Loss: 0.2207, Test Acc: 96.67%\n",
      "Epoch 346/500, Loss: 0.2201, Test Acc: 96.67%\n",
      "Epoch 347/500, Loss: 0.2194, Test Acc: 96.67%\n",
      "Epoch 348/500, Loss: 0.2187, Test Acc: 96.67%\n",
      "Epoch 349/500, Loss: 0.2181, Test Acc: 96.67%\n",
      "Epoch 350/500, Loss: 0.2174, Test Acc: 96.67%\n",
      "Epoch 351/500, Loss: 0.2168, Test Acc: 96.67%\n",
      "Epoch 352/500, Loss: 0.2161, Test Acc: 96.67%\n",
      "Epoch 353/500, Loss: 0.2155, Test Acc: 96.67%\n",
      "Epoch 354/500, Loss: 0.2148, Test Acc: 96.67%\n",
      "Epoch 355/500, Loss: 0.2142, Test Acc: 96.67%\n",
      "Epoch 356/500, Loss: 0.2136, Test Acc: 96.67%\n",
      "Epoch 357/500, Loss: 0.2129, Test Acc: 96.67%\n",
      "Epoch 358/500, Loss: 0.2123, Test Acc: 96.67%\n",
      "Epoch 359/500, Loss: 0.2117, Test Acc: 96.67%\n",
      "Epoch 360/500, Loss: 0.2110, Test Acc: 96.67%\n",
      "Epoch 361/500, Loss: 0.2104, Test Acc: 96.67%\n",
      "Epoch 362/500, Loss: 0.2098, Test Acc: 96.67%\n",
      "Epoch 363/500, Loss: 0.2092, Test Acc: 96.67%\n",
      "Epoch 364/500, Loss: 0.2086, Test Acc: 96.67%\n",
      "Epoch 365/500, Loss: 0.2080, Test Acc: 96.67%\n",
      "Epoch 366/500, Loss: 0.2074, Test Acc: 96.67%\n",
      "Epoch 367/500, Loss: 0.2068, Test Acc: 96.67%\n",
      "Epoch 368/500, Loss: 0.2062, Test Acc: 96.67%\n",
      "Epoch 369/500, Loss: 0.2056, Test Acc: 96.67%\n",
      "Epoch 370/500, Loss: 0.2050, Test Acc: 96.67%\n",
      "Epoch 371/500, Loss: 0.2044, Test Acc: 96.67%\n",
      "Epoch 372/500, Loss: 0.2038, Test Acc: 96.67%\n",
      "Epoch 373/500, Loss: 0.2032, Test Acc: 96.67%\n",
      "Epoch 374/500, Loss: 0.2026, Test Acc: 96.67%\n",
      "Epoch 375/500, Loss: 0.2021, Test Acc: 96.67%\n",
      "Epoch 376/500, Loss: 0.2015, Test Acc: 96.67%\n",
      "Epoch 377/500, Loss: 0.2009, Test Acc: 96.67%\n",
      "Epoch 378/500, Loss: 0.2003, Test Acc: 96.67%\n",
      "Epoch 379/500, Loss: 0.1998, Test Acc: 96.67%\n",
      "Epoch 380/500, Loss: 0.1992, Test Acc: 96.67%\n",
      "Epoch 381/500, Loss: 0.1987, Test Acc: 96.67%\n",
      "Epoch 382/500, Loss: 0.1981, Test Acc: 96.67%\n",
      "Epoch 383/500, Loss: 0.1975, Test Acc: 96.67%\n",
      "Epoch 384/500, Loss: 0.1970, Test Acc: 96.67%\n",
      "Epoch 385/500, Loss: 0.1964, Test Acc: 96.67%\n",
      "Epoch 386/500, Loss: 0.1959, Test Acc: 96.67%\n",
      "Epoch 387/500, Loss: 0.1954, Test Acc: 96.67%\n",
      "Epoch 388/500, Loss: 0.1948, Test Acc: 96.67%\n",
      "Epoch 389/500, Loss: 0.1943, Test Acc: 96.67%\n",
      "Epoch 390/500, Loss: 0.1937, Test Acc: 96.67%\n",
      "Epoch 391/500, Loss: 0.1932, Test Acc: 96.67%\n",
      "Epoch 392/500, Loss: 0.1927, Test Acc: 96.67%\n",
      "Epoch 393/500, Loss: 0.1922, Test Acc: 96.67%\n",
      "Epoch 394/500, Loss: 0.1916, Test Acc: 96.67%\n",
      "Epoch 395/500, Loss: 0.1911, Test Acc: 96.67%\n",
      "Epoch 396/500, Loss: 0.1906, Test Acc: 96.67%\n",
      "Epoch 397/500, Loss: 0.1901, Test Acc: 96.67%\n",
      "Epoch 398/500, Loss: 0.1896, Test Acc: 96.67%\n",
      "Epoch 399/500, Loss: 0.1890, Test Acc: 96.67%\n",
      "Epoch 400/500, Loss: 0.1885, Test Acc: 96.67%\n",
      "Epoch 401/500, Loss: 0.1880, Test Acc: 96.67%\n",
      "Epoch 402/500, Loss: 0.1875, Test Acc: 96.67%\n",
      "Epoch 403/500, Loss: 0.1870, Test Acc: 96.67%\n",
      "Epoch 404/500, Loss: 0.1865, Test Acc: 96.67%\n",
      "Epoch 405/500, Loss: 0.1860, Test Acc: 96.67%\n",
      "Epoch 406/500, Loss: 0.1855, Test Acc: 96.67%\n",
      "Epoch 407/500, Loss: 0.1851, Test Acc: 96.67%\n",
      "Epoch 408/500, Loss: 0.1846, Test Acc: 96.67%\n",
      "Epoch 409/500, Loss: 0.1841, Test Acc: 96.67%\n",
      "Epoch 410/500, Loss: 0.1836, Test Acc: 96.67%\n",
      "Epoch 411/500, Loss: 0.1831, Test Acc: 96.67%\n",
      "Epoch 412/500, Loss: 0.1826, Test Acc: 96.67%\n",
      "Epoch 413/500, Loss: 0.1822, Test Acc: 96.67%\n",
      "Epoch 414/500, Loss: 0.1817, Test Acc: 96.67%\n",
      "Epoch 415/500, Loss: 0.1812, Test Acc: 96.67%\n",
      "Epoch 416/500, Loss: 0.1807, Test Acc: 96.67%\n",
      "Epoch 417/500, Loss: 0.1803, Test Acc: 96.67%\n",
      "Epoch 418/500, Loss: 0.1798, Test Acc: 96.67%\n",
      "Epoch 419/500, Loss: 0.1794, Test Acc: 96.67%\n",
      "Epoch 420/500, Loss: 0.1789, Test Acc: 96.67%\n",
      "Epoch 421/500, Loss: 0.1784, Test Acc: 96.67%\n",
      "Epoch 422/500, Loss: 0.1780, Test Acc: 96.67%\n",
      "Epoch 423/500, Loss: 0.1775, Test Acc: 96.67%\n",
      "Epoch 424/500, Loss: 0.1771, Test Acc: 96.67%\n",
      "Epoch 425/500, Loss: 0.1766, Test Acc: 96.67%\n",
      "Epoch 426/500, Loss: 0.1762, Test Acc: 96.67%\n",
      "Epoch 427/500, Loss: 0.1757, Test Acc: 96.67%\n",
      "Epoch 428/500, Loss: 0.1753, Test Acc: 96.67%\n",
      "Epoch 429/500, Loss: 0.1749, Test Acc: 96.67%\n",
      "Epoch 430/500, Loss: 0.1744, Test Acc: 96.67%\n",
      "Epoch 431/500, Loss: 0.1740, Test Acc: 96.67%\n",
      "Epoch 432/500, Loss: 0.1735, Test Acc: 96.67%\n",
      "Epoch 433/500, Loss: 0.1731, Test Acc: 96.67%\n",
      "Epoch 434/500, Loss: 0.1727, Test Acc: 96.67%\n",
      "Epoch 435/500, Loss: 0.1723, Test Acc: 96.67%\n",
      "Epoch 436/500, Loss: 0.1718, Test Acc: 96.67%\n",
      "Epoch 437/500, Loss: 0.1714, Test Acc: 96.67%\n",
      "Epoch 438/500, Loss: 0.1710, Test Acc: 96.67%\n",
      "Epoch 439/500, Loss: 0.1706, Test Acc: 96.67%\n",
      "Epoch 440/500, Loss: 0.1702, Test Acc: 96.67%\n",
      "Epoch 441/500, Loss: 0.1697, Test Acc: 96.67%\n",
      "Epoch 442/500, Loss: 0.1693, Test Acc: 96.67%\n",
      "Epoch 443/500, Loss: 0.1689, Test Acc: 96.67%\n",
      "Epoch 444/500, Loss: 0.1685, Test Acc: 96.67%\n",
      "Epoch 445/500, Loss: 0.1681, Test Acc: 96.67%\n",
      "Epoch 446/500, Loss: 0.1677, Test Acc: 96.67%\n",
      "Epoch 447/500, Loss: 0.1673, Test Acc: 96.67%\n",
      "Epoch 448/500, Loss: 0.1669, Test Acc: 96.67%\n",
      "Epoch 449/500, Loss: 0.1665, Test Acc: 96.67%\n",
      "Epoch 450/500, Loss: 0.1661, Test Acc: 96.67%\n",
      "Epoch 451/500, Loss: 0.1657, Test Acc: 96.67%\n",
      "Epoch 452/500, Loss: 0.1653, Test Acc: 96.67%\n",
      "Epoch 453/500, Loss: 0.1649, Test Acc: 96.67%\n",
      "Epoch 454/500, Loss: 0.1645, Test Acc: 96.67%\n",
      "Epoch 455/500, Loss: 0.1641, Test Acc: 96.67%\n",
      "Epoch 456/500, Loss: 0.1638, Test Acc: 96.67%\n",
      "Epoch 457/500, Loss: 0.1634, Test Acc: 96.67%\n",
      "Epoch 458/500, Loss: 0.1630, Test Acc: 96.67%\n",
      "Epoch 459/500, Loss: 0.1626, Test Acc: 96.67%\n",
      "Epoch 460/500, Loss: 0.1622, Test Acc: 96.67%\n",
      "Epoch 461/500, Loss: 0.1619, Test Acc: 96.67%\n",
      "Epoch 462/500, Loss: 0.1615, Test Acc: 96.67%\n",
      "Epoch 463/500, Loss: 0.1611, Test Acc: 96.67%\n",
      "Epoch 464/500, Loss: 0.1607, Test Acc: 96.67%\n",
      "Epoch 465/500, Loss: 0.1604, Test Acc: 96.67%\n",
      "Epoch 466/500, Loss: 0.1600, Test Acc: 96.67%\n",
      "Epoch 467/500, Loss: 0.1596, Test Acc: 96.67%\n",
      "Epoch 468/500, Loss: 0.1593, Test Acc: 96.67%\n",
      "Epoch 469/500, Loss: 0.1589, Test Acc: 96.67%\n",
      "Epoch 470/500, Loss: 0.1586, Test Acc: 96.67%\n",
      "Epoch 471/500, Loss: 0.1582, Test Acc: 96.67%\n",
      "Epoch 472/500, Loss: 0.1578, Test Acc: 96.67%\n",
      "Epoch 473/500, Loss: 0.1575, Test Acc: 96.67%\n",
      "Epoch 474/500, Loss: 0.1571, Test Acc: 96.67%\n",
      "Epoch 475/500, Loss: 0.1568, Test Acc: 96.67%\n",
      "Epoch 476/500, Loss: 0.1564, Test Acc: 96.67%\n",
      "Epoch 477/500, Loss: 0.1561, Test Acc: 96.67%\n",
      "Epoch 478/500, Loss: 0.1557, Test Acc: 96.67%\n",
      "Epoch 479/500, Loss: 0.1554, Test Acc: 96.67%\n",
      "Epoch 480/500, Loss: 0.1550, Test Acc: 96.67%\n",
      "Epoch 481/500, Loss: 0.1547, Test Acc: 96.67%\n",
      "Epoch 482/500, Loss: 0.1544, Test Acc: 96.67%\n",
      "Epoch 483/500, Loss: 0.1540, Test Acc: 96.67%\n",
      "Epoch 484/500, Loss: 0.1537, Test Acc: 96.67%\n",
      "Epoch 485/500, Loss: 0.1533, Test Acc: 96.67%\n",
      "Epoch 486/500, Loss: 0.1530, Test Acc: 96.67%\n",
      "Epoch 487/500, Loss: 0.1527, Test Acc: 96.67%\n",
      "Epoch 488/500, Loss: 0.1523, Test Acc: 96.67%\n",
      "Epoch 489/500, Loss: 0.1520, Test Acc: 96.67%\n",
      "Epoch 490/500, Loss: 0.1517, Test Acc: 96.67%\n",
      "Epoch 491/500, Loss: 0.1514, Test Acc: 96.67%\n",
      "Epoch 492/500, Loss: 0.1510, Test Acc: 96.67%\n",
      "Epoch 493/500, Loss: 0.1507, Test Acc: 96.67%\n",
      "Epoch 494/500, Loss: 0.1504, Test Acc: 96.67%\n",
      "Epoch 495/500, Loss: 0.1501, Test Acc: 96.67%\n",
      "Epoch 496/500, Loss: 0.1497, Test Acc: 96.67%\n",
      "Epoch 497/500, Loss: 0.1494, Test Acc: 96.67%\n",
      "Epoch 498/500, Loss: 0.1491, Test Acc: 96.67%\n",
      "Epoch 499/500, Loss: 0.1488, Test Acc: 96.67%\n",
      "Epoch 500/500, Loss: 0.1485, Test Acc: 96.67%\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "for epoch in range(epochs):\n",
    "    # Forward + backward + update\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    loss = loss_fun(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Evaluasi test set\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(X_test)\n",
    "        _, predicted = torch.max(test_outputs, 1)\n",
    "        acc = (predicted == y_test).sum().item() / len(y_test)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}, Test Acc: {acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437eb159",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bc1f47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8750aa29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa65849",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85529b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ba6f7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc16674",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc9b1f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea516c45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85a1b3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3643f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e79e30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a881f2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63fcf6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecccbbdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ea4aad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59f2bfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45edaa2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9d44f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0115cfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b840c362",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a29a7d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be8f5ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
